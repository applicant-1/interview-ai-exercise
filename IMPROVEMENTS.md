# Further Features and Improvements

The following features and improvements could be added to enhance the system. They are roughly ordered by priority:

**1. Clear Product Requirements and Metrics of Success - Set Out What the System is Intended to Do:**

It is crucial to have a clear understanding of the product requirements. This includes laying out specific user stories around how the system is designed to behave in different circumstances. From this, it is then easy to derive initial evals.

If possible, it is also useful to define key metrics which can be used to measure improvements in the system along key dimensions. Without some baseline metrics, it is difficult to know whether the system is improving or not, and to identify areas for improvement.

**2. Add LLM Tracing to Improve Observability:**

It would be beneficial to implement a tracing mechanism for the LLM (Large Language Model) interactions. This would allow for better debugging and understanding of how the model(s) are making decisions based on the input they receives. It would make it easier to design and develop evals and to iterate on prompts. This could be a system like Langfuse.

**3. Build out an Initial Set of Evals:**

Without reliable evals, it is difficult to measure the performance of the system and to iterate on improvements. Building out an initial set of evals would allow for better measurement of the system's performance and would help identify areas for improvement.

Evals could be of the following types:

- End-to-end evals: These would test the entire system, from user query to final response, ensuring that the system behaves as expected. This is useful to ensuring that the system behaves consistently (i.e. that the response doesn't change unexpectedly) and for measuring various other aspects of the quality of the output.

- Component-level evals: These would test individual components of the system, such as the query formulator (currently called `rewriter`) and the response generator. This is useful to ensure that each component behaves as expected and to identify areas for improvement.

**4. Add a Reranking Step to the Retrieval Pipeline:**

Adding a reranker typically boosts the relevancy of top results returned by the retrieval system. This would involve adding a call out to, for instance, a Cohere Reranker or one of the good open source cross-encoder models.

**5. Add to the Context in the Prompts:**

The current response-generating prompts have very limited context about the user query and nature of the OpenAPI specification being queried. Improving the context in the prompts would lead to more accurate and relevant responses.

It would also be useful to move the prompts into separate markdown files so that different versions can be switched in and out easily, and so that they can be versioned controlled separately from the code logic.

**6. Tailor the Chunking and Indexing Strategy for OpenAPI Specifications:**

The current chunking strategy relies on overlapping chunks. A more sophisticated chunking strategy would rely on a much deeper understanding of the OpenAPI specification structure.

It could make sense to augment the content of the schemas for instance with longer text summaries. This way the system could use multiple indexes, relying on chunks of the schemas but also textual summaries, which could map to user queries more effectively. These additional text summaries of the schemas could be pre-generated by an LLM, based on an understanding of the schema definitions.

It would also make sense to add topics or tags (for which document it is for instance) and enable the retrieval system to rely on text-based queries as well as filters. This would ensure the system only sees information relevant to the specific specification being queried.

**7. Add Further Guardrails:**

In the interest of time, there is a very basic guardrail added to check whether the user query can be answered from the chunks of the OpenAPI specification. However, this guardrail is prone to misfire and could be improved significantly. Additional guardrails to consider would be input guardrails to ensure that the user query is valid and relevant, as well as output guardrails to ensure that the response generated is appropriate and safe.

**9. Fine-tune models on OpenAPI Specifications:**

Since the specifications are structured documents, it could make sense to fine-tune different components within the system. For instance, the embedding model or reranker model could be fine-tuned on a dataset of OpenAPI specifications to improve their performance on this specific type of data.

**10. Switch to Using Conversations Rather Than One-Off Messages:**

The current system uses one-off messages for interactions. Switching to a conversation-based model would allow for more context to be maintained across interactions, improving the relevance and accuracy of responses.

It would also lead to a more natural interaction flow between the user and the assistant.

Within this a conversation memory could be implemented to store relevant information from previous interactions, allowing the assistant to provide more contextually aware responses.

**11. Make Everything Asynchronous:**

The current system is synchronous (although with some async calls now added), which can lead to delays and inefficiencies, especially when dealing with multiple requests or long-running tasks. Making the system asynchronous by default would allow for better performance and responsiveness.

**12. Improve Greater Test Coverage, Fix Static Typing Errors:**

The current test coverage is not comprehensive, and there are some static typing errors in the codebase. Improving the test coverage would ensure that the system behaves as expected and that changes do not introduce new bugs. Fixing static typing errors would improve code quality and maintainability.
